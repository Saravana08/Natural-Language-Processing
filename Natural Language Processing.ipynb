{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Its a branch of AI that helps computers to understand, interpret and manipulate human language.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Man is to woman as king is to __________?\n",
    "# Is King to kings as the queen is to_______?\n",
    "# Meaning (king) – meaning (man) + meaning ( woman)=?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Morphological Processing\n",
    "#It is the first phase of NLP. The purpose of this phase is to break chunks of language input into sets of tokens corresponding to paragraphs, sentences and words. For example, a word like “uneasy” can be broken into two sub-word tokens as “un-easy”.\n",
    "\n",
    "# Syntax Analysis\n",
    "#It is the second phase of NLP. The purpose of this phase is two folds: to check that a sentence is well formed or not and to break it up into a structure that shows the syntactic relationships between the different words. For example, the sentence like “The school goes to the boy” would be rejected by syntax analyzer or parser.\n",
    "\n",
    "# Semantic Analysis\n",
    "#It is the third phase of NLP. The purpose of this phase is to draw exact meaning, or you can say dictionary meaning from the text. The text is checked for meaningfulness. For example, semantic analyzer would reject a sentence like “Hot ice-cream”.\n",
    "\n",
    "# Pragmatic Analysis\n",
    "#It is the fourth phase of NLP. Pragmatic analysis simply fits the actual objects/events, which exist in a given context with object references obtained during the last phase (semantic analysis). For example, the sentence “Put the banana in the basket on the shelf” can have two semantic interpretations and pragmatic analyzer will choose between these two possibilities.\n",
    "\n",
    "# Corpus\n",
    "#A corpus is a large and structured set of machine-readable texts that have been produced in a natural communicative setting. Its plural is corpora. They can be derived in different ways like text that was originally electronic, transcripts of spoken language and optical character recognition, etc.\n",
    "\n",
    "# Regular Expressions\n",
    "#A regular expression (RE) is a language for specifying text search strings. RE helps us to match or find other strings or sets of strings, using a specialized syntax held in a pattern. Regular expressions are used to search texts in UNIX as well as in MS WORD in identical way. We have various search engines using a number of RE features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['God is Great!', 'I won a lottery.']\n"
     ]
    }
   ],
   "source": [
    "# Sentence Tokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "text = \"God is Great! I won a lottery.\"\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['God', 'is', 'Great', '!', 'I', 'won', 'a', 'lottery', '.']\n"
     ]
    }
   ],
   "source": [
    "# Word Tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "text = \"God is Great! I won a lottery.\"\n",
    "tokens =word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Comp = '''A computer is an electronic device capable of performing complex calculations \n",
    "and tasks impossible for a human brain to accomplish. First ever mechanical computer was \n",
    "developed in 19th century by Charles Babbage. Since then computers have undergone many \n",
    "transformational changes in size and processing speed. Modern computers are capable of \n",
    "taking human instructions in a form of language called programming language and delivering \n",
    "output in fraction of a second.\n",
    "\n",
    "Today, computers are used in every office and institution for performing a number of tasks \n",
    "from maintaining and processing data, keeping records of transactions and employees,\n",
    "preparing and maintaining account statements, balance sheets etc. High speed computers\n",
    "are used in more complex science programs such as space exploration missions and satellite\n",
    "launch. Computers have become an integral part of our life due to its usefulness into various \n",
    "fields.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A computer is an electronic device capable of performing complex calculations \\nand tasks impossible for a human brain to accomplish.', 'First ever mechanical computer was \\ndeveloped in 19th century by Charles Babbage.', 'Since then computers have undergone many \\ntransformational changes in size and processing speed.', 'Modern computers are capable of \\ntaking human instructions in a form of language called programming language and delivering \\noutput in fraction of a second.', 'Today, computers are used in every office and institution for performing a number of tasks \\nfrom maintaining and processing data, keeping records of transactions and employees,\\npreparing and maintaining account statements, balance sheets etc.', 'High speed computers\\nare used in more complex science programs such as space exploration missions and satellite\\nlaunch.', 'Computers have become an integral part of our life due to its usefulness into various \\nfields.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "Comp_sent = sent_tokenize(Comp)\n",
    "print(Comp_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'computer', 'is', 'an', 'electronic', 'device', 'capable', 'of', 'performing', 'complex', 'calculations', 'and', 'tasks', 'impossible', 'for', 'a', 'human', 'brain', 'to', 'accomplish', '.', 'First', 'ever', 'mechanical', 'computer', 'was', 'developed', 'in', '19th', 'century', 'by', 'Charles', 'Babbage', '.', 'Since', 'then', 'computers', 'have', 'undergone', 'many', 'transformational', 'changes', 'in', 'size', 'and', 'processing', 'speed', '.', 'Modern', 'computers', 'are', 'capable', 'of', 'taking', 'human', 'instructions', 'in', 'a', 'form', 'of', 'language', 'called', 'programming', 'language', 'and', 'delivering', 'output', 'in', 'fraction', 'of', 'a', 'second', '.', 'Today', ',', 'computers', 'are', 'used', 'in', 'every', 'office', 'and', 'institution', 'for', 'performing', 'a', 'number', 'of', 'tasks', 'from', 'maintaining', 'and', 'processing', 'data', ',', 'keeping', 'records', 'of', 'transactions', 'and', 'employees', ',', 'preparing', 'and', 'maintaining', 'account', 'statements', ',', 'balance', 'sheets', 'etc', '.', 'High', 'speed', 'computers', 'are', 'used', 'in', 'more', 'complex', 'science', 'programs', 'such', 'as', 'space', 'exploration', 'missions', 'and', 'satellite', 'launch', '.', 'Computers', 'have', 'become', 'an', 'integral', 'part', 'of', 'our', 'life', 'due', 'to', 'its', 'usefulness', 'into', 'various', 'fields', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "Comp_tokens = word_tokenize(Comp)\n",
    "print(Comp_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Comp_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence tokenizing\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "word='hey i am a good guy. and want to explore nlp as well as deep learning.'\n",
    "\n",
    "sent_tokenize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word tokenizing\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokenize(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steming & Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foot\n",
      "bat\n",
      "happyily\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "  \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('feet'))\n",
    "print(lemmatizer.lemmatize('bats'))\n",
    "print(lemmatizer.lemmatize('happyily'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "God\n",
      "is\n",
      "Great\n",
      "!\n",
      "I\n",
      "won\n",
      "a\n",
      "lottery\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for i in tokens:\n",
    "    lemmatizer.lemmatize(i)\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference\n",
    "\n",
    "#https://en.wikipedia.org/wiki/Stemming\n",
    "#http://www.nltk.org/install.html\n",
    "#https://text-processing.com/demo/stem/\n",
    "#http://people.scs.carleton.ca/~armyunis/projects/KAPI/porter.pdf\n",
    "#https://en.wikipedia.org/wiki/Document_clustering\n",
    "#https://en.wikipedia.org/wiki/Text_mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fairy'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lematizing the words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lem=WordNetLemmatizer()\n",
    "\n",
    "wordss= ('fairies')\n",
    "\n",
    "lem.lemmatize(wordss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'striped', 'bats', 'are', 'hanging', 'on', 'their', 'feet', 'for', 'best']\n",
      "The striped bat are hanging on their foot for best\n"
     ]
    }
   ],
   "source": [
    "# Define the sentence to be lemmatized\n",
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "# Tokenize: Split the sentence into words\n",
    "word_list = nltk.word_tokenize(sentence)\n",
    "print(word_list)\n",
    "#> ['The', 'striped', 'bats', 'are', 'hanging', 'on', 'their', 'feet', 'for', 'best']\n",
    "\n",
    "# Lemmatize list of words and join\n",
    "lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "print(lemmatized_output)\n",
    "#> The striped bat are hanging on their foot for best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "program  :  program\n",
      "programs  :  program\n",
      "programer  :  programer\n",
      "programing  :  programing\n",
      "programers  :  programers\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize \n",
    "   \n",
    "ps = WordNetLemmatizer() \n",
    "  \n",
    "# choose some words to be stemmed \n",
    "words = [\"program\", \"programs\", \"programer\", \"programing\", \"programers\"] \n",
    "  \n",
    "for w in words: \n",
    "    print(w, \" : \", ps.lemmatize(w)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affections, Affects, Affected, Affection, Affecting --- Affect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "pst = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem('having')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'institut'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem('institutions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give\n",
      "give\n",
      "gave\n",
      "given\n"
     ]
    }
   ],
   "source": [
    "stemmed_words = ['give', 'giving', 'gave', 'given']\n",
    "\n",
    "for i in stemmed_words:\n",
    "    print(pst.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "englishStemmer=SnowballStemmer(\"english\")\n",
    "englishStemmer.stem(\"having\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Porter Stemmer      lancaster Stemmer   \n",
      "friend              friend              friend              \n",
      "friendship          friendship          friend              \n",
      "friends             friend              friend              \n",
      "friendships         friendship          friend              \n",
      "stabil              stabil              stabl               \n",
      "destabilize         destabil            dest                \n",
      "misunderstanding    misunderstand       misunderstand       \n",
      "railroad            railroad            railroad            \n",
      "moonlight           moonlight           moonlight           \n",
      "football            footbal             footbal             \n"
     ]
    }
   ],
   "source": [
    "#A list of words to be stemmed\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "lancaster=LancasterStemmer()\n",
    "\n",
    "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\",\"stabil\",\"destabilize\",\"misunderstanding\",\"railroad\",\"moonlight\",\"football\"]\n",
    "print(\"{0:20}{1:20}{2:20}\".format(\"Word\",\"Porter Stemmer\",\"lancaster Stemmer\"))\n",
    "for word in word_list:\n",
    "    print(\"{0:20}{1:20}{2:20}\".format(word,porter.stem(word),lancaster.stem(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activ\n",
      "activ\n",
      "activ\n",
      "fairi\n"
     ]
    }
   ],
   "source": [
    "# stemming the words\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "ps=PorterStemmer()\n",
    "\n",
    "example=['activate','activation','activating','fairies']\n",
    "\n",
    "for i in example:\n",
    "    print(ps.stem(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "fdist = FreqDist()\n",
    "\n",
    "for i in Comp_tokens:\n",
    "    fdist[i.lower()]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Counter.most_common of FreqDist({'and': 8, 'of': 7, '.': 7, 'in': 6, 'a': 5, 'computers': 5, ',': 4, 'are': 3, 'computer': 2, 'an': 2, ...})>\n"
     ]
    }
   ],
   "source": [
    "print(fdist.most_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('and', 8), ('of', 7), ('.', 7), ('in', 6), ('a', 5), ('computers', 5), (',', 4), ('are', 3), ('computer', 2), ('an', 2), ('capable', 2), ('performing', 2), ('complex', 2), ('tasks', 2), ('for', 2), ('human', 2), ('to', 2), ('have', 2), ('processing', 2), ('speed', 2)]\n"
     ]
    }
   ],
   "source": [
    "print(fdist.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram, Trigram & Ngram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigrams - Tokens of two consecutive words are Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigrams - Tokens of three consecutive words are Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ngrams - Tokens of any no of consecitive words are Ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import bigrams,trigrams,ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = '''Natural language processing (NLP) is a subfield of linguistics, computer science, \n",
    "information engineering, and artificial intelligence concerned with the interactions between \n",
    "computers and human (natural) languages, in particular how to program computers to process \n",
    "and analyze large amounts of natural language data.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'linguistics', ',', 'computer', 'science', ',', 'information', 'engineering', ',', 'and', 'artificial', 'intelligence', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', '(', 'natural', ')', 'languages', ',', 'in', 'particular', 'how', 'to', 'program', 'computers', 'to', 'process', 'and', 'analyze', 'large', 'amounts', 'of', 'natural', 'language', 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence_tokens = word_tokenize(sentence)\n",
    "print(sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Natural', 'language'),\n",
       " ('language', 'processing'),\n",
       " ('processing', '('),\n",
       " ('(', 'NLP'),\n",
       " ('NLP', ')'),\n",
       " (')', 'is'),\n",
       " ('is', 'a'),\n",
       " ('a', 'subfield'),\n",
       " ('subfield', 'of'),\n",
       " ('of', 'linguistics'),\n",
       " ('linguistics', ','),\n",
       " (',', 'computer'),\n",
       " ('computer', 'science'),\n",
       " ('science', ','),\n",
       " (',', 'information'),\n",
       " ('information', 'engineering'),\n",
       " ('engineering', ','),\n",
       " (',', 'and'),\n",
       " ('and', 'artificial'),\n",
       " ('artificial', 'intelligence'),\n",
       " ('intelligence', 'concerned'),\n",
       " ('concerned', 'with'),\n",
       " ('with', 'the'),\n",
       " ('the', 'interactions'),\n",
       " ('interactions', 'between'),\n",
       " ('between', 'computers'),\n",
       " ('computers', 'and'),\n",
       " ('and', 'human'),\n",
       " ('human', '('),\n",
       " ('(', 'natural'),\n",
       " ('natural', ')'),\n",
       " (')', 'languages'),\n",
       " ('languages', ','),\n",
       " (',', 'in'),\n",
       " ('in', 'particular'),\n",
       " ('particular', 'how'),\n",
       " ('how', 'to'),\n",
       " ('to', 'program'),\n",
       " ('program', 'computers'),\n",
       " ('computers', 'to'),\n",
       " ('to', 'process'),\n",
       " ('process', 'and'),\n",
       " ('and', 'analyze'),\n",
       " ('analyze', 'large'),\n",
       " ('large', 'amounts'),\n",
       " ('amounts', 'of'),\n",
       " ('of', 'natural'),\n",
       " ('natural', 'language'),\n",
       " ('language', 'data'),\n",
       " ('data', '.')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_bigrams = list(bigrams(sentence_tokens))\n",
    "sentence_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Natural', 'language', 'processing'),\n",
       " ('language', 'processing', '('),\n",
       " ('processing', '(', 'NLP'),\n",
       " ('(', 'NLP', ')'),\n",
       " ('NLP', ')', 'is'),\n",
       " (')', 'is', 'a'),\n",
       " ('is', 'a', 'subfield'),\n",
       " ('a', 'subfield', 'of'),\n",
       " ('subfield', 'of', 'linguistics'),\n",
       " ('of', 'linguistics', ','),\n",
       " ('linguistics', ',', 'computer'),\n",
       " (',', 'computer', 'science'),\n",
       " ('computer', 'science', ','),\n",
       " ('science', ',', 'information'),\n",
       " (',', 'information', 'engineering'),\n",
       " ('information', 'engineering', ','),\n",
       " ('engineering', ',', 'and'),\n",
       " (',', 'and', 'artificial'),\n",
       " ('and', 'artificial', 'intelligence'),\n",
       " ('artificial', 'intelligence', 'concerned'),\n",
       " ('intelligence', 'concerned', 'with'),\n",
       " ('concerned', 'with', 'the'),\n",
       " ('with', 'the', 'interactions'),\n",
       " ('the', 'interactions', 'between'),\n",
       " ('interactions', 'between', 'computers'),\n",
       " ('between', 'computers', 'and'),\n",
       " ('computers', 'and', 'human'),\n",
       " ('and', 'human', '('),\n",
       " ('human', '(', 'natural'),\n",
       " ('(', 'natural', ')'),\n",
       " ('natural', ')', 'languages'),\n",
       " (')', 'languages', ','),\n",
       " ('languages', ',', 'in'),\n",
       " (',', 'in', 'particular'),\n",
       " ('in', 'particular', 'how'),\n",
       " ('particular', 'how', 'to'),\n",
       " ('how', 'to', 'program'),\n",
       " ('to', 'program', 'computers'),\n",
       " ('program', 'computers', 'to'),\n",
       " ('computers', 'to', 'process'),\n",
       " ('to', 'process', 'and'),\n",
       " ('process', 'and', 'analyze'),\n",
       " ('and', 'analyze', 'large'),\n",
       " ('analyze', 'large', 'amounts'),\n",
       " ('large', 'amounts', 'of'),\n",
       " ('amounts', 'of', 'natural'),\n",
       " ('of', 'natural', 'language'),\n",
       " ('natural', 'language', 'data'),\n",
       " ('language', 'data', '.')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_trigram = list(trigrams(sentence_tokens))\n",
    "sentence_trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Natural', 'language', 'processing', '(', 'NLP', ')'),\n",
       " ('language', 'processing', '(', 'NLP', ')', 'is'),\n",
       " ('processing', '(', 'NLP', ')', 'is', 'a'),\n",
       " ('(', 'NLP', ')', 'is', 'a', 'subfield'),\n",
       " ('NLP', ')', 'is', 'a', 'subfield', 'of'),\n",
       " (')', 'is', 'a', 'subfield', 'of', 'linguistics'),\n",
       " ('is', 'a', 'subfield', 'of', 'linguistics', ','),\n",
       " ('a', 'subfield', 'of', 'linguistics', ',', 'computer'),\n",
       " ('subfield', 'of', 'linguistics', ',', 'computer', 'science'),\n",
       " ('of', 'linguistics', ',', 'computer', 'science', ','),\n",
       " ('linguistics', ',', 'computer', 'science', ',', 'information'),\n",
       " (',', 'computer', 'science', ',', 'information', 'engineering'),\n",
       " ('computer', 'science', ',', 'information', 'engineering', ','),\n",
       " ('science', ',', 'information', 'engineering', ',', 'and'),\n",
       " (',', 'information', 'engineering', ',', 'and', 'artificial'),\n",
       " ('information', 'engineering', ',', 'and', 'artificial', 'intelligence'),\n",
       " ('engineering', ',', 'and', 'artificial', 'intelligence', 'concerned'),\n",
       " (',', 'and', 'artificial', 'intelligence', 'concerned', 'with'),\n",
       " ('and', 'artificial', 'intelligence', 'concerned', 'with', 'the'),\n",
       " ('artificial', 'intelligence', 'concerned', 'with', 'the', 'interactions'),\n",
       " ('intelligence', 'concerned', 'with', 'the', 'interactions', 'between'),\n",
       " ('concerned', 'with', 'the', 'interactions', 'between', 'computers'),\n",
       " ('with', 'the', 'interactions', 'between', 'computers', 'and'),\n",
       " ('the', 'interactions', 'between', 'computers', 'and', 'human'),\n",
       " ('interactions', 'between', 'computers', 'and', 'human', '('),\n",
       " ('between', 'computers', 'and', 'human', '(', 'natural'),\n",
       " ('computers', 'and', 'human', '(', 'natural', ')'),\n",
       " ('and', 'human', '(', 'natural', ')', 'languages'),\n",
       " ('human', '(', 'natural', ')', 'languages', ','),\n",
       " ('(', 'natural', ')', 'languages', ',', 'in'),\n",
       " ('natural', ')', 'languages', ',', 'in', 'particular'),\n",
       " (')', 'languages', ',', 'in', 'particular', 'how'),\n",
       " ('languages', ',', 'in', 'particular', 'how', 'to'),\n",
       " (',', 'in', 'particular', 'how', 'to', 'program'),\n",
       " ('in', 'particular', 'how', 'to', 'program', 'computers'),\n",
       " ('particular', 'how', 'to', 'program', 'computers', 'to'),\n",
       " ('how', 'to', 'program', 'computers', 'to', 'process'),\n",
       " ('to', 'program', 'computers', 'to', 'process', 'and'),\n",
       " ('program', 'computers', 'to', 'process', 'and', 'analyze'),\n",
       " ('computers', 'to', 'process', 'and', 'analyze', 'large'),\n",
       " ('to', 'process', 'and', 'analyze', 'large', 'amounts'),\n",
       " ('process', 'and', 'analyze', 'large', 'amounts', 'of'),\n",
       " ('and', 'analyze', 'large', 'amounts', 'of', 'natural'),\n",
       " ('analyze', 'large', 'amounts', 'of', 'natural', 'language'),\n",
       " ('large', 'amounts', 'of', 'natural', 'language', 'data'),\n",
       " ('amounts', 'of', 'natural', 'language', 'data', '.')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_ngrams = list(ngrams(sentence_tokens,6))\n",
    "sentence_ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords# # Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopword = stopwords.words('english')\n",
    "len(stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "computer\n",
      "electronic\n",
      "device\n",
      "capable\n",
      "performing\n",
      "complex\n",
      "calculations\n",
      "tasks\n",
      "impossible\n",
      "human\n",
      "brain\n",
      "accomplish\n",
      ".\n",
      "First\n",
      "ever\n",
      "mechanical\n",
      "computer\n",
      "developed\n",
      "19th\n",
      "century\n",
      "Charles\n",
      "Babbage\n",
      ".\n",
      "Since\n",
      "computers\n",
      "undergone\n",
      "many\n",
      "transformational\n",
      "changes\n",
      "size\n",
      "processing\n",
      "speed\n",
      ".\n",
      "Modern\n",
      "computers\n",
      "capable\n",
      "taking\n",
      "human\n",
      "instructions\n",
      "form\n",
      "language\n",
      "called\n",
      "programming\n",
      "language\n",
      "delivering\n",
      "output\n",
      "fraction\n",
      "second\n",
      ".\n",
      "Today\n",
      ",\n",
      "computers\n",
      "used\n",
      "every\n",
      "office\n",
      "institution\n",
      "performing\n",
      "number\n",
      "tasks\n",
      "maintaining\n",
      "processing\n",
      "data\n",
      ",\n",
      "keeping\n",
      "records\n",
      "transactions\n",
      "employees\n",
      ",\n",
      "preparing\n",
      "maintaining\n",
      "account\n",
      "statements\n",
      ",\n",
      "balance\n",
      "sheets\n",
      "etc\n",
      ".\n",
      "High\n",
      "speed\n",
      "computers\n",
      "used\n",
      "complex\n",
      "science\n",
      "programs\n",
      "space\n",
      "exploration\n",
      "missions\n",
      "satellite\n",
      "launch\n",
      ".\n",
      "Computers\n",
      "become\n",
      "integral\n",
      "part\n",
      "life\n",
      "due\n",
      "usefulness\n",
      "various\n",
      "fields\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for i in Comp_tokens:\n",
    "    if i not in stopword:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['football']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stopwords list\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "stopwords = set(stopwords.words('English'))\n",
    "wors=word_tokenize(word)\n",
    "filtered=[]\n",
    "\n",
    "for i in wors:\n",
    "    if i not in stopwords:\n",
    "        filtered.append(i)\n",
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sample text'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove noise from the data\n",
    "\n",
    "noise_list = [\"is\", \"a\", \"this\", \"...\"] \n",
    "\n",
    "def remove_noise(input_text):\n",
    "    words = input_text.split() \n",
    "    noise_free_words = [word for word in words if word not in noise_list] \n",
    "    noise_free_text = \" \".join(noise_free_words) \n",
    "    return noise_free_text\n",
    "\n",
    "remove_noise(\"this is a sample text\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'remove this  from analytics vidhya'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove regex\n",
    "import re \n",
    "\n",
    "def remove_regex(input_text, regex_pattern):\n",
    "    urls = re.finditer(regex_pattern, input_text) \n",
    "    for i in urls: \n",
    "        input_text = re.sub(i.group().strip(), '', input_text)\n",
    "    return input_text\n",
    "\n",
    "regex_pattern = \"#[\\w]*\"  \n",
    "\n",
    "remove_regex(\"remove this #hashtag from analytics vidhya\", regex_pattern)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parts of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My', 'name', 'is', 'Rony']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "sentence = \"My name is Rony\"\n",
    "token = nltk.word_tokenize(sentence)\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('My', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('Rony', 'NNP')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset(\"NN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "para = '''A well-organized paragraph supports or develops a single controlling idea, which is expressed in a sentence called the topic sentence. A topic sentence has several important functions: it substantiates or supports an essay’s thesis statement; it unifies the content of a paragraph and directs the order of the sentences; and it advises the reader of the subject to be discussed and how the paragraph will discuss it. Readers generally look to the first few sentences in a paragraph to determine the subject and perspective of the paragraph. That’s why it’s often best to put the topic sentence at the very beginning of the paragraph. In some cases, however, it’s more effective to place another sentence before the topic sentence—for example, a sentence linking the current paragraph to the previous one, or one providing background information.\n",
    "\n",
    "Although most paragraphs should have a topic sentence, there are a few situations when a paragraph might not need a topic sentence. For example, you might be able to omit a topic sentence in a paragraph that narrates a series of events, if a paragraph continues developing an idea that you introduced (with a topic sentence) in the previous paragraph, or if all the sentences and details in a paragraph clearly refer—perhaps indirectly—to a main point. The vast majority of your paragraphs, however, should have a topic sentence.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A well-organized paragraph supports or develops a single controlling idea, which is expressed in a sentence called the topic sentence.',\n",
       " 'A topic sentence has several important functions: it substantiates or supports an essay’s thesis statement; it unifies the content of a paragraph and directs the order of the sentences; and it advises the reader of the subject to be discussed and how the paragraph will discuss it.',\n",
       " 'Readers generally look to the first few sentences in a paragraph to determine the subject and perspective of the paragraph.',\n",
       " 'That’s why it’s often best to put the topic sentence at the very beginning of the paragraph.',\n",
       " 'In some cases, however, it’s more effective to place another sentence before the topic sentence—for example, a sentence linking the current paragraph to the previous one, or one providing background information.',\n",
       " 'Although most paragraphs should have a topic sentence, there are a few situations when a paragraph might not need a topic sentence.',\n",
       " 'For example, you might be able to omit a topic sentence in a paragraph that narrates a series of events, if a paragraph continues developing an idea that you introduced (with a topic sentence) in the previous paragraph, or if all the sentences and details in a paragraph clearly refer—perhaps indirectly—to a main point.',\n",
       " 'The vast majority of your paragraphs, however, should have a topic sentence.']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "wordss = word_tokenize(para)\n",
    "len(wordss)\n",
    "words = [word for word in wordss if word.isalpha()]\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean=[]\n",
    "for i in words:\n",
    "        clean.append(i.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean =set(clean)\n",
    "len(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = []\n",
    "for i in clean:\n",
    "    if i not in stop:\n",
    "        st.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(st)\n",
    "st = str(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wor = WordNetLemmatizer()\n",
    "words= wor.lemmatize(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['first', 'majority', 'statement', 'vast', 'developing', 'series', 'another', 'functions', 'events', 'might', 'place', 'example', 'previous', 'expressed', 'put', 'continues', 'main', 'cases', 'sentences', 'single', 'important', 'beginning', 'thesis', 'linking', 'discuss', 'point', 'best', 'providing', 'several', 'details', 'called', 'introduced', 'idea', 'however', 'need', 'although', 'effective', 'sentence', 'unifies', 'generally', 'clearly', 'supports', 'look', 'topic', 'readers', 'one', 'advises', 'often', 'directs', 'order', 'essay', 'subject', 'substantiates', 'current', 'discussed', 'paragraph', 'background', 'content', 'information', 'perspective', 'controlling', 'paragraphs', 'omit', 'develops', 'situations', 'determine', 'able', 'reader', 'narrates']\""
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ste=PorterStemmer()\n",
    "stemed = ste.stem(st)\n",
    "stemed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = nltk.pos_tag(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " ('f', 'NN'),\n",
       " ('i', 'NN'),\n",
       " ('r', 'VBP'),\n",
       " ('s', 'JJ'),\n",
       " ('t', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('m', 'NN'),\n",
       " ('a', 'DT'),\n",
       " ('j', 'NN'),\n",
       " ('o', 'IN'),\n",
       " ('r', 'NN'),\n",
       " ('i', 'NN'),\n",
       " ('t', 'VBP'),\n",
       " ('y', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('s', 'NN'),\n",
       " ('t', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('t', 'NN'),\n",
       " ('e', 'NN'),\n",
       " ('m', 'NN'),\n",
       " ('e', 'NN'),\n",
       " ('n', 'JJ'),\n",
       " ('t', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('v', 'NN'),\n",
       " ('a', 'DT'),\n",
       " ('s', 'JJ'),\n",
       " ('t', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('d', 'NN'),\n",
       " ('e', 'NN'),\n",
       " ('v', 'NN'),\n",
       " ('e', 'NN'),\n",
       " ('l', 'NN'),\n",
       " ('o', 'IN'),\n",
       " ('p', 'NN'),\n",
       " ('i', 'NN'),\n",
       " ('n', 'VBP'),\n",
       " ('g', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('s', 'NN'),\n",
       " ('e', 'IN'),\n",
       " ('r', 'NN'),\n",
       " ('i', 'NN'),\n",
       " ('e', 'VBP'),\n",
       " ('s', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('a', 'DT'),\n",
       " ('n', 'JJ'),\n",
       " ('o', 'NN'),\n",
       " ('t', 'NN'),\n",
       " ('h', 'NN'),\n",
       " ('e', 'NN'),\n",
       " ('r', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('f', 'NN'),\n",
       " ('u', 'JJ'),\n",
       " ('n', 'JJ'),\n",
       " ('c', 'NN'),\n",
       " ('t', 'NN'),\n",
       " ('i', 'NN'),\n",
       " ('o', 'VBP'),\n",
       " ('n', 'RB'),\n",
       " ('s', 'JJ'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('e', 'NN'),\n",
       " ('v', 'NNS'),\n",
       " ('e', 'VBP'),\n",
       " ('n', 'JJ'),\n",
       " ('t', 'NN'),\n",
       " ('s', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('m', 'NN'),\n",
       " ('i', 'NN'),\n",
       " ('g', 'VBP'),\n",
       " ('h', 'NN'),\n",
       " ('t', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('p', 'NN'),\n",
       " ('l', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('c', 'JJ'),\n",
       " ('e', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('e', 'NN'),\n",
       " ('x', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('m', 'JJ'),\n",
       " ('p', 'NN'),\n",
       " ('l', 'NN'),\n",
       " ('e', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('p', 'NN'),\n",
       " ('r', 'NN'),\n",
       " ('e', 'NN'),\n",
       " ('v', 'NN'),\n",
       " ('i', 'NN'),\n",
       " ('o', 'VBP'),\n",
       " ('u', 'JJ'),\n",
       " ('s', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('e', 'NN'),\n",
       " ('x', 'NNP'),\n",
       " ('p', 'NN'),\n",
       " ('r', 'NN'),\n",
       " ('e', 'NN'),\n",
       " ('s', 'NN'),\n",
       " ('s', 'NN'),\n",
       " ('e', 'NN'),\n",
       " ('d', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('p', 'NN'),\n",
       " ('u', 'JJ'),\n",
       " ('t', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('c', 'NN'),\n",
       " ('o', 'IN'),\n",
       " ('n', 'JJ'),\n",
       " ('t', 'NN'),\n",
       " ('i', 'NN'),\n",
       " ('n', 'VBP'),\n",
       " ('u', 'JJ'),\n",
       " ('e', 'NN'),\n",
       " ('s', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('m', 'NN'),\n",
       " ('a', 'DT'),\n",
       " ('i', 'JJ'),\n",
       " ('n', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('c', 'NN'),\n",
       " ('a', 'DT'),\n",
       " ('s', 'JJ'),\n",
       " ('e', 'NN'),\n",
       " ('s', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('s', 'NN'),\n",
       " ('e', 'NN'),\n",
       " ('n', 'JJ'),\n",
       " ('t', 'NN'),\n",
       " ('e', 'NN'),\n",
       " ('n', 'JJ'),\n",
       " ('c', 'NN'),\n",
       " ('e', 'NN'),\n",
       " ('s', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('s', 'NN'),\n",
       " ('i', 'NN'),\n",
       " ('n', 'VBP'),\n",
       " ('g', 'NN'),\n",
       " ('l', 'NN'),\n",
       " ('e', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('i', 'NN'),\n",
       " ('m', 'VBP'),\n",
       " ('p', 'NN'),\n",
       " ('o', 'NN'),\n",
       " ('r', 'NN'),\n",
       " ('t', 'VBD'),\n",
       " ('a', 'DT'),\n",
       " ('n', 'JJ'),\n",
       " ('t', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('b', 'NN'),\n",
       " ('e', 'NN'),\n",
       " ('g', 'NN'),\n",
       " ('i', 'NN'),\n",
       " ('n', 'VBP'),\n",
       " ('n', 'NN'),\n",
       " ('i', 'NN'),\n",
       " ('n', 'VBP'),\n",
       " ('g', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('t', 'NN'),\n",
       " ('h', 'NN'),\n",
       " ('e', 'NN'),\n",
       " ('s', 'NN'),\n",
       " ('i', 'NN'),\n",
       " ('s', 'VBP'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('l', 'NN'),\n",
       " ('i', 'NN'),\n",
       " ('n', 'VBP'),\n",
       " ('k', 'NN'),\n",
       " ('i', 'NN'),\n",
       " ('n', 'VBP'),\n",
       " ('g', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('d', 'NN'),\n",
       " ('i', 'NN'),\n",
       " ('s', 'VBP'),\n",
       " ('c', 'NN'),\n",
       " ('u', 'JJ'),\n",
       " ('s', 'JJ'),\n",
       " ('s', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('p', 'NN'),\n",
       " ('o', 'NN'),\n",
       " ('i', 'NN'),\n",
       " ('n', 'VBP'),\n",
       " ('t', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('b', 'NN'),\n",
       " ('e', 'NN'),\n",
       " ('s', 'NN'),\n",
       " ('t', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('p', 'NN'),\n",
       " ('r', 'NN'),\n",
       " ('o', 'IN'),\n",
       " ('v', 'NN'),\n",
       " ('i', 'NN'),\n",
       " ('d', 'VBP'),\n",
       " ('i', 'NN'),\n",
       " ('n', 'VBP'),\n",
       " ('g', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('s', 'NN'),\n",
       " ('e', 'NN'),\n",
       " ('v', 'NN'),\n",
       " ('e', 'NN'),\n",
       " ('r', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('l', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('d', 'NN'),\n",
       " ('e', 'NN'),\n",
       " ('t', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('i', 'JJ'),\n",
       " ('l', 'NN'),\n",
       " ('s', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('c', 'NN'),\n",
       " ('a', 'DT'),\n",
       " ('l', 'NN'),\n",
       " ('l', 'NN'),\n",
       " ('e', 'NN'),\n",
       " ('d', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('i', 'NN'),\n",
       " ('n', 'VBP'),\n",
       " ('t', 'NN'),\n",
       " ('r', 'NN'),\n",
       " ('o', 'NN'),\n",
       " ('d', 'NN'),\n",
       " ('u', 'JJ'),\n",
       " ('c', 'NN'),\n",
       " ('e', 'NN'),\n",
       " ('d', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('i', 'NN'),\n",
       " ('d', 'VBP'),\n",
       " ('e', 'IN'),\n",
       " ('a', 'DT'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('h', 'NN'),\n",
       " ('o', 'IN'),\n",
       " ('w', 'JJ'),\n",
       " ('e', 'NN'),\n",
       " ('v', 'NN'),\n",
       " ('e', 'NN'),\n",
       " ('r', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('n', 'JJ'),\n",
       " ('e', 'NN'),\n",
       " ('e', 'NN'),\n",
       " ('d', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('a', 'DT'),\n",
       " ('l', 'NN'),\n",
       " ('t', 'NN'),\n",
       " ('h', 'NN'),\n",
       " ('o', 'JJ'),\n",
       " ('u', 'JJ'),\n",
       " ('g', 'NN'),\n",
       " ('h', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('e', 'NN'),\n",
       " ('f', 'NNS'),\n",
       " ('f', 'VBP'),\n",
       " ('e', 'JJ'),\n",
       " ('c', 'JJ'),\n",
       " ('t', 'NN'),\n",
       " ('i', 'NN'),\n",
       " ('v', 'VBP'),\n",
       " ('e', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('s', 'NN'),\n",
       " ('e', 'NN'),\n",
       " ('n', 'JJ'),\n",
       " ('t', 'NN'),\n",
       " ('e', 'NN'),\n",
       " ('n', 'JJ'),\n",
       " ('c', 'NN'),\n",
       " ('e', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('u', 'JJ'),\n",
       " ('n', 'NN'),\n",
       " ('i', 'NN'),\n",
       " ('f', 'VBP'),\n",
       " ('i', 'NN'),\n",
       " ('e', 'VBP'),\n",
       " ('s', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('g', 'NN'),\n",
       " ('e', 'NN'),\n",
       " ('n', 'JJ'),\n",
       " ('e', 'NN'),\n",
       " ('r', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('l', 'NN'),\n",
       " ('l', 'NN'),\n",
       " ('y', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('c', 'NN'),\n",
       " ('l', 'NN'),\n",
       " ('e', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('r', 'NN'),\n",
       " ('l', 'NN'),\n",
       " ('y', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('s', 'NN'),\n",
       " ('u', 'JJ'),\n",
       " ('p', 'NN'),\n",
       " ('p', 'NN'),\n",
       " ('o', 'NN'),\n",
       " ('r', 'NN'),\n",
       " ('t', 'NN'),\n",
       " ('s', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('l', 'NN'),\n",
       " ('o', 'IN'),\n",
       " ('o', 'JJ'),\n",
       " ('k', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('t', 'NN'),\n",
       " ('o', 'IN'),\n",
       " ('p', 'NN'),\n",
       " ('i', 'NN'),\n",
       " ('c', 'VBP'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('r', 'NN'),\n",
       " ('e', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('d', 'NN'),\n",
       " ('e', 'NN'),\n",
       " ('r', 'NN'),\n",
       " ('s', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('o', 'NN'),\n",
       " ('n', 'IN'),\n",
       " ('e', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('a', 'DT'),\n",
       " ('d', 'NN'),\n",
       " ('v', 'NN'),\n",
       " ('i', 'NN'),\n",
       " ('s', 'VBP'),\n",
       " ('e', 'NN'),\n",
       " ('s', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('o', 'NN'),\n",
       " ('f', 'NN'),\n",
       " ('t', 'NN'),\n",
       " ('e', 'NN'),\n",
       " ('n', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('d', 'NN'),\n",
       " ('i', 'NN'),\n",
       " ('r', 'VBP'),\n",
       " ('e', 'NN'),\n",
       " ('c', 'NNS'),\n",
       " ('t', 'VBP'),\n",
       " ('s', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('o', 'NN'),\n",
       " ('r', 'NN'),\n",
       " ('d', 'NN'),\n",
       " ('e', 'NN'),\n",
       " ('r', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('e', 'NN'),\n",
       " ('s', 'NNS'),\n",
       " ('s', 'VBP'),\n",
       " ('a', 'DT'),\n",
       " ('y', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('s', 'NN'),\n",
       " ('u', 'NN'),\n",
       " ('b', 'NN'),\n",
       " ('j', 'NN'),\n",
       " ('e', 'NN'),\n",
       " ('c', 'VBP'),\n",
       " ('t', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('s', 'NN'),\n",
       " ('u', 'NN'),\n",
       " ('b', 'NN'),\n",
       " ('s', 'NN'),\n",
       " ('t', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('n', 'JJ'),\n",
       " ('t', 'NN'),\n",
       " ('i', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('t', 'NN'),\n",
       " ('e', 'NN'),\n",
       " ('s', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('c', 'NN'),\n",
       " ('u', 'NN'),\n",
       " ('r', 'NN'),\n",
       " ('r', 'NN'),\n",
       " ('e', 'NN'),\n",
       " ('n', 'JJ'),\n",
       " ('t', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('d', 'NN'),\n",
       " ('i', 'NN'),\n",
       " ('s', 'VBP'),\n",
       " ('c', 'NN'),\n",
       " ('u', 'JJ'),\n",
       " ('s', 'NN'),\n",
       " ('s', 'NN'),\n",
       " ('e', 'NN'),\n",
       " ('d', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('p', 'NN'),\n",
       " ('a', 'DT'),\n",
       " ('r', 'NN'),\n",
       " ('a', 'DT'),\n",
       " ('g', 'NN'),\n",
       " ('r', 'NN'),\n",
       " ('a', 'DT'),\n",
       " ('p', 'NN'),\n",
       " ('h', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('b', 'NN'),\n",
       " ('a', 'DT'),\n",
       " ('c', 'NN'),\n",
       " ('k', 'NN'),\n",
       " ('g', 'NN'),\n",
       " ('r', 'NN'),\n",
       " ('o', 'NN'),\n",
       " ('u', 'JJ'),\n",
       " ('n', 'JJ'),\n",
       " ('d', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('c', 'NN'),\n",
       " ('o', 'IN'),\n",
       " ('n', 'JJ'),\n",
       " ('t', 'NN'),\n",
       " ('e', 'NN'),\n",
       " ('n', 'JJ'),\n",
       " ('t', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('i', 'NN'),\n",
       " ('n', 'VBP'),\n",
       " ('f', 'NN'),\n",
       " ('o', 'NN'),\n",
       " ('r', 'NN'),\n",
       " ('m', 'VBD'),\n",
       " ('a', 'DT'),\n",
       " ('t', 'NN'),\n",
       " ('i', 'NN'),\n",
       " ('o', 'VBP'),\n",
       " ('n', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('p', 'NN'),\n",
       " ('e', 'NN'),\n",
       " ('r', 'NN'),\n",
       " ('s', 'NN'),\n",
       " ('p', 'NN'),\n",
       " ('e', 'NN'),\n",
       " ('c', 'VBP'),\n",
       " ('t', 'NN'),\n",
       " ('i', 'NN'),\n",
       " ('v', 'VBP'),\n",
       " ('e', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('c', 'NN'),\n",
       " ('o', 'IN'),\n",
       " ('n', 'JJ'),\n",
       " ('t', 'NN'),\n",
       " ('r', 'NN'),\n",
       " ('o', 'NN'),\n",
       " ('l', 'NN'),\n",
       " ('l', 'NN'),\n",
       " ('i', 'NN'),\n",
       " ('n', 'VBP'),\n",
       " ('g', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('p', 'NN'),\n",
       " ('a', 'DT'),\n",
       " ('r', 'NN'),\n",
       " ('a', 'DT'),\n",
       " ('g', 'NN'),\n",
       " ('r', 'NN'),\n",
       " ('a', 'DT'),\n",
       " ('p', 'NN'),\n",
       " ('h', 'NN'),\n",
       " ('s', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('o', 'JJ'),\n",
       " ('m', 'NN'),\n",
       " ('i', 'NN'),\n",
       " ('t', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('d', 'NN'),\n",
       " ('e', 'NN'),\n",
       " ('v', 'NN'),\n",
       " ('e', 'NN'),\n",
       " ('l', 'NN'),\n",
       " ('o', 'NN'),\n",
       " ('p', 'NN'),\n",
       " ('s', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('s', 'NN'),\n",
       " ('i', 'NN'),\n",
       " ('t', 'VBP'),\n",
       " ('u', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('t', 'NN'),\n",
       " ('i', 'NN'),\n",
       " ('o', 'VBP'),\n",
       " ('n', 'RB'),\n",
       " ('s', 'JJ'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('d', 'NN'),\n",
       " ('e', 'NN'),\n",
       " ('t', 'NN'),\n",
       " ('e', 'NN'),\n",
       " ('r', 'NN'),\n",
       " ('m', 'NN'),\n",
       " ('i', 'NN'),\n",
       " ('n', 'VBP'),\n",
       " ('e', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('a', 'DT'),\n",
       " ('b', 'NN'),\n",
       " ('l', 'NN'),\n",
       " ('e', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('r', 'NN'),\n",
       " ('e', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('d', 'NN'),\n",
       " ('e', 'NN'),\n",
       " ('r', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " (',', ','),\n",
       " (' ', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('n', 'NN'),\n",
       " ('a', 'DT'),\n",
       " ('r', 'NN'),\n",
       " ('r', 'NN'),\n",
       " ('a', 'DT'),\n",
       " ('t', 'NN'),\n",
       " ('e', 'NN'),\n",
       " ('s', 'NN'),\n",
       " (\"'\", 'POS'),\n",
       " (']', 'NN')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset(\"JJ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Noun (N)- Daniel, London, table, dog, teacher, pen, city, happiness, hope\n",
    "#Verb (V)- go, speak, run, eat, play, live, walk, have, like, are, is\n",
    "#Adjective(ADJ)- big, happy, green, young, fun, crazy, three\n",
    "#Adverb(ADV)- slowly, quietly, very, always, never, too, well, tomorrow\n",
    "#Preposition (P)- at, on, in, from, with, near, between, about, under\n",
    "#Conjunction (CON)- and, or, but, because, so, yet, unless, since, if\n",
    "#Pronoun(PRO)- I, you, we, they, he, she, it, me, us, them, him, her, this\n",
    "#Interjection (INT)- Ouch! Wow! Great! Help! Oh! Hey! Hi!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = nltk.pos_tag(token)\n",
    "chunks = ne_chunk(tag)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = '''European authorities fined Google a record $5.1 billion on Wednesday for abusing its \n",
    "power in the mobile phone market and ordered the company to alter its practices'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sent):\n",
    "    sent = nltk.word_tokenize(sent)\n",
    "    sent = nltk.pos_tag(sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = preprocess(ex)\n",
    "sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to provide the correct POS tag\n",
    "# https://www.clips.uantwerpen.be/pages/MBSP-tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Sukanya', 'NNP'), ('Rajib', 'NNP'), ('and', 'CC'), ('Naba', 'NNP'), ('are', 'VBP'), ('my', 'PRP$'), ('good', 'JJ'), ('friends', 'NNS'), ('Sukanya', 'NNP'), ('is', 'VBZ'), ('getting', 'VBG'), ('married', 'VBN'), ('next', 'JJ'), ('year', 'NN'), ('Marriage', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('big', 'JJ'), ('step', 'NN'), ('in', 'IN'), ('one', 'CD'), ('’', 'NN'), ('s', 'NN'), ('life.It', 'NN'), ('is', 'VBZ'), ('both', 'DT'), ('exciting', 'VBG'), ('and', 'CC'), ('frightening', 'NN'), ('But', 'CC'), ('friendship', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('sacred', 'JJ'), ('bond', 'NN'), ('between', 'IN'), ('people.It', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('special', 'JJ'), ('kind', 'NN'), ('of', 'IN'), ('love', 'NN'), ('between', 'IN'), ('us', 'PRP'), ('Many', 'JJ'), ('of', 'IN'), ('you', 'PRP'), ('must', 'MD'), ('have', 'VB'), ('tried', 'VBN'), ('searching', 'VBG'), ('for', 'IN'), ('a', 'DT'), ('friend', 'NN'), ('but', 'CC'), ('never', 'RB'), ('found', 'VBD'), ('the', 'DT'), ('right', 'JJ'), ('one', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob \n",
    "  \n",
    "text = (\"Sukanya, Rajib and Naba are my good friends. \" +\n",
    "    \"Sukanya is getting married next year. \" +\n",
    "    \"Marriage is a big step in one’s life.\" +\n",
    "    \"It is both exciting and frightening. \" + \n",
    "    \"But friendship is a sacred bond between people.\" +\n",
    "    \"It is a special kind of love between us. \" +\n",
    "    \"Many of you must have tried searching for a friend \"+ \n",
    "    \"but never found the right one.\") \n",
    "  \n",
    "# create a textblob object \n",
    "blob_object = TextBlob(text) \n",
    "  \n",
    "# Part-of-speech tags can be accessed  \n",
    "# through the tags property of blob object.' \n",
    "  \n",
    "# print word with pos tag. \n",
    "print(blob_object.tags) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "# create the transform\n",
    "vectorizer_cv = CountVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer_cv.fit(text)\n",
    "# summarize\n",
    "print(vectorizer_cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[1 1 1 1 1 1 1 2]]\n"
     ]
    }
   ],
   "source": [
    "# encode document\n",
    "vector = vectorizer_cv.transform(text)\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(type(vector))\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# encode another document\n",
    "text2 = [\"the puppy\"]\n",
    "vector1 = vectorizer_cv.transform(text2)\n",
    "print(vector1.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 0 0 0 1 2]]\n"
     ]
    }
   ],
   "source": [
    "# encode another document\n",
    "text3 = [\"the puppy said to the quick brown fox\"]\n",
    "vector2 = vectorizer_cv.transform(text3)\n",
    "print(vector2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer_cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFidf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
      "[1.69314718 1.28768207 1.28768207 1.69314718 1.69314718 1.69314718\n",
      " 1.69314718 1.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\",\n",
    "\t\t\"The dog.\",\n",
    "\t\t\"The fox\"]\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8)\n",
      "[[0.36388646 0.27674503 0.27674503 0.36388646 0.36388646 0.36388646\n",
      "  0.36388646 0.42983441]]\n"
     ]
    }
   ],
   "source": [
    "# encode document\n",
    "vector = vectorizer.transform([text[0]])\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
